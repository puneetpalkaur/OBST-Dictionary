When the great Dane of 20th century physics, Niels Bohr, was not busy chewing on a juicy morsel of quantum mechanics, he was known to yap away witticisms worthy of Yogi Berra. The classic Bohrism “Prediction is difficult, especially about the future” alas came too late to save Lord Kelvin. Just as physics was set to debut in Einstein's own production of Extreme Makeover, Kelvin judged the time ripe to pen the field's obituary: “There is nothing new to be discovered in physics now.” Not his lordship's finest hour.
Nor his worst. Aware that fallibility is the concession the genius makes to common mortals to keep them from despairing, Kelvin set early on to give the mortals much to be hopeful about. To wit, the thermodynamics pioneer devoted the first half of his life to studying hot air and the latter half to blowing it. Ever the perfectionist, he elevated to an art form the production of pure, unadulterated bunk: “X-rays will prove to be a hoax”; “Radio has no future”; “Heavier-than-air flying machines are impossible”; and my personal favorite, “In science there is only physics; all the rest is stamp collecting.” Kelvin's crystal ball was the gift that kept on giving.


Gloat not at a genius' misfortunes. Futurologitis is an equal-opportunity affliction, one hardly confined to the physicist's ward. “I think there is a world market for maybe five computers,” averred IBM Chairman, Thomas Watson, a gem of prescience matched only by a 1939 New York Times editorial: “The problem with television is that people must sit and keep their eyes glued to the screen; the average American family hasn't time for it.” The great demographer Thomas Malthus owes much of his fame to his loopy prediction that exponentially increasing populations would soon outrun the food supply. As the apprentice soothsayer learns in “Crystal Gazing 101,” never predict a geometric growth!
Apparently, Gordon Moore skipped that class. In 1965, the co-founder of semiconductor giant Intel announced his celebrated law: Computing power doubles every two years. Moore's Law has, if anything, erred on the conservative side. Every eighteen months, an enigmatic pagan ritual will see white-robed sorcerers silently shuffle into a temple dedicated to the god of cleanliness, and soon reemerge with, on their faces, a triumphant smile and, in their hands, a silicon wafer twice as densely packed as the day before. No commensurate growth in human mental powers has been observed: this has left us scratching our nonexpanding heads, wondering what it is we've done to deserve such luck.
To get a feel for the magic, consider that the latest Sony PlayStation would easily outpace the fastest supercomputer from the early nineties. If not for Moore's Law, the Information Superhighway would be a back alley to Snoozeville; the coolest thing about the computer would still be the blinking lights. And so, next time you ask who engineered the digital revolution, expect many hands to rise. But watch the long arm of Moore's Law tower above all others. Whatever your brand of high-tech addiction, be it IM, iPod, YouTube, or Xbox, be aware that you owe it first and foremost to the engineering wizardry that has sustained Moore's predictive prowess over the past forty years.
Enjoy it while it lasts, because it won't. Within a few decades, say the optimists, a repeal is all but certain. Taking their cue from Bill Gates, the naysayers conjure up the curse of power dissipation, among other woes, to declare Moore's Law in the early stage of rigor mortis. Facing the bleak, sorrowful tomorrows of  The Incredible Shrinking Chip That Won't Shrink No More,  what's a computer scientist to do?


Break out the Dom and pop the corks, of course! Moore's Law has added fizz and sparkle to the computing cocktail, but for too long its exhilarating potency has distracted the party-goers from their Holy Grail quest: How to unleash the full computing and modeling power of the Algorithm. Not to stretch the metaphor past its snapping point, the temptation is there for the Algorithmistas (my tribe) to fancy themselves as the Knights of the Round Table and look down on Moore's Law as the Killer Rabbit, viciously elbowing King Arthur's intrepid algorithmic warriors. Just as an abundance of cheap oil has delayed the emergence of smart energy alternatives, Moore's Law has kept algorithms off center stage. Paradoxically, it has also been their enabler: the killer bunny turned sacrificial rabbit who sets the track champion on a world record pace, only to fade into oblivion once the trophy has been handed out. With the fading imminent, it is not too soon to ask why the Algorithm is destined to achieve celebrity status within the larger world of science. While you ask, let me boldly plant the flag and bellow the battle cry:
“The Algorithm's coming-of-age as the new language of science promises to be the most disruptive scientific development since quantum mechanics.”
If you think such a blinding flare of hyperbole surely blazed right out of Lord Kelvin's crystal ball, read on and think again. A computer is a storyteller and algorithms are its tales. We'll get to the tales in a minute but, first, a few words about the storytelling.
Computing is the meeting point of three powerful concepts: universality, duality, and self-reference. In the modern era, this triumvirate has bowed to the class-conscious influence of the tractability creed. The creed's incessant call to complexity class warfare has, in turn, led to the emergence of that ultimate class leveler: the Algorithm. Today, not only is this new “order” empowering the e-technology that stealthily rules our lives; it is also challenging what we mean by knowing, believing, trusting, persuading, and learning. No less. Some say the Algorithm is poised to become the new New Math, the idiom of modern science. I say The Sciences They Are A-Changin' and the Algorithm is Here to Stay.
Reread the previous paragraph. If it still looks like a glorious goulash of blathering nonsense, good! I shall now explain, so buckle up!



The universal computer
 
Had Thomas Jefferson been a computer scientist, school children across the land would rise in the morning and chant these hallowed words:
“We hold these truths to be self-evident, that all computers are created equal, that they are endowed by their Creator with certain unalienable Rights, that among these are Universality and the separation of Data, Control, and Command.”
Computers come in different shapes, sizes, and colors, but all are created equal—indeed, much like 18th century white male American colonists. Whatever the world's fastest supercomputer can do (in 2006, that would be the IBM Blue Gene/L), your lowly iPod can do it, too, albeit a little more slowly. Where it counts, size doesn't matter: all computers are qualitatively the same. Even exotic beasts such as quantum computers, vector machines, DNA computers, and cellular automata can all be viewed as fancy iPods. That's universality!


Here's how it works. Your iPod is a tripod (where did you think they got that name?), with three legs called control, program, and data. Together, the program and the data form the two sections of a document [program | data] that, to the untrained eye, resembles a giant, amorphous string of 0s and 1s. Something like this:
[ 1110001010110010010 | 1010111010101001110 ]
Each section has its own, distinct purpose: the program specifies instructions for the control to follow (eg, how to convert text into pdf); the data encodes plain information, like this essay (no, not plain in thatsense). The data string is to be read, not to be read into. About it, Freud would have quipped: “Sometimes a string is just a string.” But he would have heard, seeping from the chambers of a program, the distant echoes of a dream: jumbled signs crying out for interpretation. To paraphrase the Talmudic saying, an uninterpreted program is like an unread letter. The beauty of the scheme is that the control need not know a thing about music. In fact, simply by downloading the appropriate program-data document, you can turn your iPod into: an earthquake simulator; a word processor; a web browser; or, if downloading is too much, a paperweight. Your dainty little MP3 player is a universal computer.
The control is the computer's brain and the sole link between program and data. Its only function in life is to read the data, interpret the program's orders, and act on them—a task so pedestrian that modern theory of reincarnation ranks the control as the lowest life form on the planet, right behind the inventor of the CD plastic wrap. If you smash your iPod open with a mallet and peek into its control, you'll discover what a marvel of electronics it is—okay, was. Even more marvelous is the fact that it need not be so. It takes little brainpower to follow orders blindly (in fact, too much tends to get in the way). Stretching this principle to the limit, one can design a universal computer with a control mechanism so simple that any old cuckoo clock will outsmart it. This begs the obvious question: did Orson Welles know that when he dissed the Swiss and their cuckoo clocks in “The Third Man”? It also raises a suspicion: doesn't the control need to add, multiply, divide, and do the sort of fancy footwork that would sorely test the nimblest of cuckoo clocks?


No. The control on your laptop might indeed do all of those things, but the point is that it need not do so. (Just as a bank might give you a toaster when you open a new account, but it need not be a toaster; it could be a pet hamster.) Want to add? Write a program to add. Want to divide? Write a program to divide. Want to print? Write a program to print. A control that delegates all it can to the program's authority will get away with a mere two dozen different “states”—simplicity a cuckoo clock could only envy. If you want your computer to do something for you, don't just scream at the control: write down instructions in the program section. Want to catch trouts? Fine, append a fishing manual to the program string. The great nutritionist Confucius said it better: “Give a man a fish and you feed him for a day. Teach a man to fish and you feed him for a lifetime.” The binary view of fishing = river + fisherman makes way for a universal one: fishing = river + fishing manual + you. Similarly,
computing = data + program + control.
This tripodal equation launched a scientific revolution, and it is to British mathematician Alan Turing that fell the honor of designing the launching pad. His genius was to let robots break out of the traditional binary brain-brawn mold, which conflates control and program, and embrace the liberating “tripod-iPod” view of computing. Adding a third leg to the robotic biped ushered in the era of universality: any computer could now simulate any other one.
Underpinning all of that, of course, was the digital representation of information: DVD vs VCR tape; piano vs violin; Anna Karenina vs Mona Lisa. The analog world of celluloid film and vinyl music is unfit for reproduction: doesn't die; just fades away. Quite the opposite, encoding information over an alphabet opens the door to unlimited, decay-free replication. In a universe of 0s and 1s, we catch a glimpse of immortality; we behold the gilded gates of eternity flung wide open by the bewitching magic of a lonely pair of incandescent symbols. In short, analog sucks, digital rocks.



Two sides of the same coin

Load your iPod with the program-data document [Print this | Print this]. Ready? Press the start button and watch the words “Print this” flash across the screen. Exciting, no? While you compose yourself with bated breath amid the gasps and the shrieks, take stock of what happened. To the unschooled novice, data and program may be identical strings, but to the cuckoo-like control they couldn't be more different: the data is no more than what it is; the program is no less than what it means. The control may choose to look at the string “Print this” either as a meaningless sequence of letters or as an order to commit ink to paper. To scan symbols mulishly or to deforest the land: that is the option at hand here—we call it duality.
So 1907, I almost hear you sigh. In that fateful year, Ferdinand de Saussure, the father of linguistics, announced to a throng of admirers that there are two sides to a linguistic sign: its signifier (representation) and its signified (interpretation). A string is a sign that, under the watchful eye of the control, acts as signifier when data and as signified when a program.
Saussure's intellectual progeny is a breed of scholars known as semioticians. Funny that linguists, of all people, would choose for themselves a name that rhymes with mortician. Funny or not, semiotics mavens will point out the imperfect symmetry between program and data. The latter is inviolate. Signifiers must be treated with the utmost reverence: they could be passwords, hip-hop rhymes, or newfound biblical commandments. Mess with them at your own peril.
Programs are different. The encoding of the signified is wholly conventional. Take the program “Print this”, for example. A francophonic control would have no problem with “Imprimer ceci ” or, for that matter, with the obsequious “O, control highly esteemed, may you, noblest of cuckoos, indulge my impudent wish to see this humble string printed out, before my cup runneth over and your battery runneth out.” The plethora of programming languages reveals how so many ways there are of signifying the same thing. (Just as the plethora of political speeches reveals how so many ways there are of signifying nothing.)

Sensing the comic, artistic, and scholarly potential of the duality between program and data, great minds went to work. Abbott and Costello's “Who's on First?” routine is built around the confusion between a baseball player's nickname (the signifier) and the pronoun “who” (the signified). Magritte's celebrated painting “Ceci n'est pas une pipe” (this is not a pipe) plays on the distinction between the picture of a pipe (the signifier) and a pipe one smokes (the signified). The great painter might as well have scribbled on a blank canvas: “Le signifiant n'est pas le signifié ” (the signifier is not the signified). But he didn't, and for that we're all grateful.
English scholars are not spared the slings and arrows of duality either. How more dual can it get than the question that keeps Elizabethan lit gurus awake at night: “Did Shakespeare write Shakespeare?” And pity the dually tormented soul that would dream up such wacky folderol: “Twas brillig, and the slithy toves Did gyre and gimble in the wabe; All mimsy were the borogoves, And the mome raths outgrabe.”



Say it ain't true
 
I am lying. Really? Then I am lying when I say I am lying; therefore, I am not lying. Yikes. But if I am not lying then I am not lying when I say I am lying; therefore, I am lying. Double yikes. Not enough yet? Okay, consider the immortal quip of the great American philosopher Homer Simpson: “Oh Marge, cartoons don't have any deep meaning; they're just stupid drawings that give you a cheap laugh.” If cartoons don't have meaning, then Homer's statement is meaningless (not merely a philosopher, the man is a cartoon character); therefore, for all we know, cartoons have meaning. But then Homer's point is... Doh! Just say it ain't true. Ain't true? No, please, don't say it ain't true! Because if it ain't true then ain't true ain't true, and so...
AAARRRGGGHHH !!!!!!
Beware of self-referencing, that is to say, of sentences that make statements about themselves. Two of the finest mathematical minds in history, Cantor and Gödel, failed to heed that advice and both went stark raving bonkers. As the Viennese gentleman with the shawl-draped couch already knew, self-reference is the quickest route to irreversible dementia.


It is also the salt of the computing earth. Load up your iPod, this time with the program-data document [Print this twice | Print this twice]. Push the start button and see the screen light up with the words: “Print this twice Print this twice”. Lo and behold, the thing prints itself! Well, not quite: the vertical bar is missing. To get everything right and put on fast track your career as a budding computer virus artist, try this instead: [Print this twice, starting with a vertical bar the second time | Print this twice, starting with a vertical bar the second time]. See how much better it works now! The key word in the self-printing business is “twice”: “never” would never work; “once” would be once too few; “thrice”?? Please watch your language.
Self-reproduction requires a tightly choreographed dance between: (i) a program explaining how to copy the data; (ii) a data string describing that very same program. By duality, the same sequence of words (or bits) is interpreted in two different ways; by self-reference, the duality coin looks the same on both sides. Self-reference—called recursion in computer parlance—requires duality; not the other way around. Which is why the universal computer owes its existence to duality and its power to recursion. If Moore's Law is the fuel of Google, recursion is its engine.
The tripodal view of computing was the major insight of Alan Turing—well, besides this little codebreaking thing he did in Bletchley Park that helped win World War II. Not to discount the lush choral voices of Princeton virtuosos Alonzo Church, Kurt Gödel, and John von Neumann, it is Maestro Turing who turned into a perfect opus the hitherto disjointed scores of the computing genre.
Mother Nature, of course, scooped them all by a few billion years. Your genome consists of two parallel strands of DNA that encode all of your genetic inheritance. Your morning addiction to Cocoa Puffs, your night cravings for Twinkies? Yep, it's all in there. Now if you take the two strands apart and line them up, you'll get two strings about three billion letters long. Check it out:
[ ACGGTATCCGAATGC...  |   TGCCATAGGCTTACG... ]
There they are: two twin siblings locking horns in a futile attempt to look different. Futile because if you flip the As into Ts and the Cs into Gs (and vice versa) you'll see each strand morph into the other one. The two strings are the same in disguise. So flip one of them to get a more symmetric layout. Like this:
[ ACGGTATCCGAATGC...  |   ACGGTATCCGAATGC... ]
Was I the only one to spot a suspicious similarity with [Print this twice | Print this twice] or did you, too? Both are program-data documents that provide perfectly yummy recipes for self-reproduction. Life's but a walking shadow, said Macbeth. Wrong. Life's but a self-printing iPod! Ministry-of-Virtue officials will bang on preachily about there being more to human life than the blind pursuit of self-replication, a silly notion that Hollywood's typical fare swats away daily at a theater near you. Existential angst aside, the string “ACGGTATCCGAATGC...” is either plain data (the genes constituting your DNA) or a program whose execution produces, among other things, all the proteins needed for DNA replication, plus all of the others needed for the far more demanding task of sustaining your Cocoa Puffs addiction. Duality is the choice you have to think of your genome either as a long polymer of nucleotides (the data to be read) or as the sequence of amino acids forming its associated proteins (the “programs of life”). Hence the fundamental equation of biology:
Life = Duality + Self-reference


On April 25, 1953, the British journal Nature published a short article whose understated punchline was the shot heard 'round the world: “It has not escaped our notice that the specific pairing we have postulated immediately suggests a possible copying mechanism for the genetic material.” In unveiling to the world the molecular structure of DNA, James Watson and Francis Crick broke the Code of Life. In so doing, they laid bare the primordial link between life and computing. One can easily imagine the reaction of that other codebreaker from Bletchley Park: “Duality and self-reference embedded in molecules? Jolly good to know God thinks like me.”
Turing's swagger would have been forgivable. After all, here was the man who had invented the computer. Here was the man who had put the mind-matter question on a scientific footing. Here was the man who had saved Britain from defeat in 1941 by breaking the Nazi code. Alas, good deeds rarely go unpunished. In lieu of a knighthood, a grateful nation awarded Alan Turing a one-way ticket to Palookaville, England: a court conviction for homosexuality with a sentence of forced estrogen injections. On June 7, 1954, barely one year to the day of Watson and Crick's triumph, Alan Turing went home, ate an apple laced with cyanide, and died. His mother believed, as a mother would, that it was an accident.



The modern era
 
The post-Turing years saw the emergence of a new computing paradigm: tractability. Its origin lay in the intuitive notion that checking a proof of Archimedes's theorem can't be nearly as hard as finding it in the first place; enjoying a coke must be simpler than discovering its secret recipe (or so the Coca Cola Company hopes), falling under the spell of 'Round Midnight ought to be easier than matching Monk's composing prowess. But is it really? Amazingly, no one knows. Welcome to the foremost open question in all of computer science!
Ever wondered whether the 1,000-song library stored in your iPod could be reordered and split up to form two equal-time playlists? Probably not. But suppose you wanted to transfer your songs to the two sides of an extra-length cassette while indulging your lifelong passion for saving money on magnetic tape. Which songs would you put on which side so as to use as little tape as possible? Now you'd be wondering, wouldn't you? (Humor me: say yes.)


You wouldn't wonder long, anyway. After a minute's reflection, you'd realize you didn't have the faintest idea how to do that. (Warning: splitting a tune in the middle is a no-no.) Of course, you could try all possibilities but that's a big number—roughly 1 followed by 300 zeroes. Ah, but your amazing friend Alice, she knows! Or so she says. Then why not just get the two playlists from her? By adding up a few numbers, you'll easily verify that she's not lying and that, indeed, both lists have the same playing time. What Alice will hand you over is, in essence, a proof that your song library can be split evenly. Your job will be reduced to that of proof-checking, a task at which a compulsive tape-saving Scrooge might even shine. Heads-up: did you notice my nonchalant use of the word “lying”? When a movie's opening scene casually trains the camera on a gun, no one might get hurt for a while, but you know that won't last.
Alas, wondrous Alice fell down the rabbit hole eons ago and, these days, a good library splitting friend is hard to find. And so, sadly, you'll have little choice but to compile the two lists yourself and engage in that dreaded thing called proof-finding. That's a tougher nut to crack. So much so that even if you were to harness the full power of an IBM Blue Gene/L running the best software available anywhere on earth and beyond, the entire lifetime of the universe wouldn't be enough! You might get lucky with the parameters and get it done sooner, but getting lucky? Yeah, right...
To add insult to injury, computer scientists have catalogued thousands of such Jurassic problems—so named for the dinosaur-like quality of their solutions: hard to discover but impossible to miss when they pop up in front of you; in other words, proofs hopelessly difficult to find but a breeze to verify. Courtesy of Murphy's Law, of course, the great Jurassics of the world include all of the hydra-headed monsters we're so desperate to slay: drug design; protein folding; resource allocation; portfolio optimization; suitcase packing; etc. Furthermore, even shooting for good approximate solutions—when the notion makes sense—can sometimes be just as daunting.
Now a funny thing happened on the way back from the word factory. Despite its dazzling lyricism, metaphorical felicity, hip-hoppish élan, not to mention a Niagara of adulatory gushing I'll kindly spare you, my staggeringly brilliant coinage “Jurassic” hasn't caught on. Yet. Skittish computer scientists tend to favor the achingly dull “NP-complete.” Worse, their idea of bustin' a dope, def funky rhyme is to—get this—write down the thing in full, as in “complete for nondeterministic polynomial time.” To each their own.
Back to the Jurassics. Always basking in the spotlight, they are famously difficult, impossibly hard to satisfy, and—if their resilience is any guide—quite pleased with the attention. These traits often run in the family; sure enough, the Jurassics are blood kin. The first to put them on the analyst's couch and pin their intractable behavior on consanguinity were Stephen Cook, Jack Edmonds, Richard Karp, and Leonid Levin. In the process they redefined computing around the notion of tractability and produced the most influential milestone in post-Turing computer science.
But what is a tractable problem, you ask? Answer: one that can be solved in polynomial time. Oh, swell; nothing like calling upon the opaque to come to the rescue of the obscure! Relax: it's quite simple, really. If you double the size of the problem—say, your iPod library will now hold 2,000 tunes instead of a mere 1,000—then the time to find an even split should at most double, or quadruple, or increase by some fixed rate (ie, independent of the problem size). That's what it means to be tractable. Convoluted as this definition may seem, it has two things going for it: one is to match our intuition of what can be solved in practice (assuming the fixed rate isn't “fixed” too high); the other is to leave the particular computer we're working on out of the picture. See how there is no mention of computing speeds; only of growth rates. It is a statement about software, not hardware. Tractability is a universal attribute of a problem—or lack thereof. Note: some scholars prefer the word feasibility. Obviously, to resist the lure of the opening riff of Wittgenstein's “Tractatus Logico-Philosophicus” takes willpower; predictably, the feasibility crowd is thin.


Library splitting does not appear to be tractable. (Hold the tears: you'll need them in a minute.) Any algorithm humans have ever tried—and many have—requires exponential time. Read: all of them share the dubious distinction that their running times get squared (not merely scaled up by a constant factor) whenever one doubles the size of the problem. If you do the math, you'll see it's the sort of growth that quickly gets out of hand.
Well, do the math. Say you want to solve a problem that involves 100 numbers and the best method in existence takes one second on your laptop. How long would it take to solve the same problem with 200 numbers, instead? Answer: just a few seconds if it's tractable; and C× 2200 = (C× 2100)2100 = 2100seconds if it's not. That's more than a billion trillion years! To paraphrase Senator Dirksen from the great State of Illinois, a trillion years here, a trillion years there, and pretty soon you're talking real time. Exponentialitis is not a pretty condition. Sadly, it afflicts the entire Jurassic menagerie.
The true nature of the ailment eludes us but this much we know: it's genetic. If any one of the Jurassics is tractable, wonder of wonders, all of them are. Better still: a cure for any one of them could easily be used to heal any of the others. Viewed through the tractability lens, the Jurassics are the same T. rex in different brontosaurus' clothings. Heady stuff! The day Alice can split your song library within a few hours will be the day biologists can fold proteins over breakfast, design new drugs by lunch, and eradicate deadly diseases just in time for dinner. The attendant medical revolution will likely make you live the long, jolly life of a giant Galápagos tortoise (life span: 150 years). Alice's discovery would imply the tractability of all the Jurassics (P=NP in computer lingo). Should the computing gods smile upon us, the practical consequences could be huge.
Granted, there would be a few losers: mostly online shoppers and mathematicians. All commercial transactions on the Internet would cease to be secure and e-business would grind to a halt. (More on this gripping drama in the next section.) The math world would take a hit, too: P=NP would prove Andrew Wiles, the conqueror of Fermat's Last Theorem, no more deserving of credit than his referee. Well, not quite. Mathematicians like to assign two purposes to a proof: one is to convince them that something is true; the other is to help them understand why something is true. Tractability bears no relevance to the latter. Still, no one wants to see friendly mathematicians swell the ranks of the unemployed as they get replaced by nano iPods, so the consensus has emerged that P is not NP. There are other reasons, too, but that one is the best because it puts computer scientists in a good light. The truth is, no one has a clue.
To be P or not to be P, that is NP's question. A million-dollar question, in fact. That's how much prize money the Clay Mathematics Institute will award Alice if she resolves the tractability of library splitting. (She will also be shipped to Guantánamo by the CIA, but that's a different essay.) Which side of the NP question should we root for? We know the stakes: a short existence blessed with online shopping (P?NP); or the interminable, eBay-less life of a giant tortoise (P=NP). Tough call.



P=NP   (Or why you won't find the proof on eBay)
 
An algorithm proving P=NP might conceivably do for technology what the discovery of the wheel did for land transportation. Granted, to discover the wheel is always nice, but to roll logs in the mud has its charms, too. Likewise, the intractability of proof-finding would have its benefits. That 1951 vintage Wham-O hula hoop you bought on eBay the other day, er, you didn't think the auction was secure just because online thieves were too hip for hula hoops, did you? What kept them at bay was the (much hoped-for) intractability of integer factorization.
Say what? Prime numbers deterring crooks? Indeed. Take two primes, S and T, each one, say, a thousand-digit long. The product R= S × T is about 2,000 digits long. Given S and T, your laptop will churn out R in a flash. On the other hand, if you knew only R, how hard would it be for you to retrieve S and T? Hard. Very hard. Very very hard. Repeat this until you believe it because the same algorithm that would find S and T could be used to steal your credit card off the Internet!


Am I implying that computer security is premised on our inability to do some silly arithmetic fast enough? I surely am. If the Jurassics were shown to be tractable, not a single computer security system would be safe. Which is why for eBay to auction off a proof of P=NP would be suicidal. Worse: factoring is not even known—or, for that matter, thought—to be one of the Jurassics. It could well be a cuddly pet dinosaur eager to please its master (if only its master had the brains to see that). One cannot rule out the existence of a fast factoring algorithm that would have no incidence on the P=NP question.
In fact, such an algorithm exists. All of the recent hoopla about quantum computing owes to the collective panic caused by Peter Shor's discovery that factoring is tractable on a quantum iPod. That building the thing itself is proving quite hopeless has helped to calm the frayed nerves of computer security experts. And yet there remains the spine-chilling possibility that maybe, just maybe, factoring is doable in practice on a humble laptop. Paranoid security pros might want to hold on to their prozac a while longer.
Cryptology is a two-faced Janus. One side studies how to decrypt the secret messages that bad people exchange among one another. That's cryptanalysis: think Nazi code, Bletchley Park, victory parade, streamers, confetti, sex, booze, and rock 'n' roll. The other branch of the field, cryptography, seeks clever ways of encoding secret messages for good people to send to other good people, so that bad people get denied the streamers, the confetti, and all the rest. Much of computer security relies on public-key cryptography. The idea is for, say, eBay to post an encryption algorithm on the web that everybody can use. When you are ready to purchase that hula hoop, you'll type in your credit card information into your computer, encrypt it right there, and then send the resulting gobbledygook over the Internet. Naturally, the folks at eBay will need their own secretdecryption algorithm to make sense of the junk they'll receive from you. (Whereas poor taste is all you'll need to make sense of the junk you'll receive from them.) The punchline is that no one should be able to decrypt anything unless they have that secret algorithm in their possession.


Easier said than done. Consider the fiendishly clever algorithm that encodes the first two words of this sentence as dpotjefs uif. So easy to encrypt: just replace each letter in the text by the next one in the alphabet. Now assume you knew this encryption scheme. How in the world would you go about decrypting a message? Ah, this is where algorithmic genius kicks in. (Algorithmistas get paid the big bucks for a reason.) It's a bit technical so I'll write slowly: replace each letter in the ciphertext by the previous one in the alphabet. Ingenious, no? And fast, too! The only problem with the system is that superior minds can crack it. So is there a cryptographic scheme that is unbreakable, irrespective of how many geniuses roam the earth? It should be child's play to go one way (encrypt) but a gargantuan undertaking to go back (decrypt)—unless, that is, one knows the decryption algorithm, in which case it should be a cinch.
RSA, named after Ron Rivest, Adi Shamir, and Len Adleman, is just such a scheme. It's an exceedingly clever, elegant public-key cryptosystem that, amazingly, requires only multiplication and long division. It rules e-commerce and pops up in countless security applications. Its universal acclaim got its inventors the Turing award (the “Nobel prize” of computer science). More important, it got Rivest a chance to throw the ceremonial first pitch for the first Red Sox-Yankees game of the 2004 season. Yes, RSA is that big! There is one catch, though (pun intended): if factoring proves to be tractable then it's bye-bye RSA, hello shopping mall.



The computational art of persuasion

Isn't intractability just a variant of undecidability, the mother's milk of logicians? One notion evokes billions of years, the other eternity—what's the difference? Whether the execution of [program | data] ever terminates is undecidable. In other words, one cannot hope to find out by writing another program and reading the output of [another program | [program | data]]. Of side interest, note how the whole document [program | data] is now treated as mere data: an artful cadenza from Maestro Turing.
Very nice, but how's undecidability helping us go through life with a smile on our face? It doesn't. In fact, no one ever tried to benefit from an undecidable problem who didn't wind up slouched face down on the Viennese gentleman's couch. Not so with intractable problems. Just as quantum mechanics shattered the platonic view of a reality amenable to noninvasive observation, tractability has clobbered classical notions of identity, randomness, and knowledge. And that's a good thing.
Why? Let me hereby declare two objects to be “identical” if to tell them apart is intractable, regardless of how different they might actually be. A deck of cards will be “perfectly” shuffled if it's impossible to prove it otherwise in polynomial time. It is one of the sweet ironies of computing that the existence of an intractable world out there makes our life down here so much easier. Think of it as the Olympics in reverse: if you can't run the 100-meter dash under 10 seconds, you win the gold!
Scientists of all stripes are insatiable consumers of random numbers: try taking a poll, conducting clinical trials, or running a lottery without them! To produce randomness can be quite arduous. To this day, only two methods have been scientifically validated. One of them is the infamous “Kitty Flop.” Strap buttered toast to the back of a cat and drop the animal from a PETA-approved height: if the butter hits the ground, record a 1; else a 0. For more bits, repeat. Randomness results from the tension between Murphy's law and the feline penchant for landing on one's feet. The other method is the classical “Coriolis Flush.” This time, go to the equator and flush the toilet: if the water whirls clockwise, your random bit is a 1; else it's a 0.
Now think how much easier it'd be if cheating were allowed. Not even bad plumbing could stop you (though many hope it would). Okay, your numbers are not truly random and your cards are not properly shuffled, but if to show they are not is intractable then why should you care? Hardness creates easiness. Of course, computer scientists have simply rediscovered what professional cyclists have known for years: the irresistible lure of intractability (of drug detection).
You're not thinking, I hope, that this is all perched on the same moral high ground as Don Corleone's philosophy that crime is not breaking the law but getting caught. If you are, will you please learn to think positive? Our take on intractability is really no different from the 1894 Supreme Court decision in Coffin vs US that introduced to American jurisprudence the maxim “Innocent until proven guilty.” Reality is not what is but what can be proven to be (with bounded patience). If you think this sort of tractability-induced relativism takes us down the garden path, think again. It actually cleanses classical notions of serious defects.
Take knowledge, for example: here's something far more faith-based than we'd like to admit. We “know” that the speed of light is constant, but who among us has actually bothered to measure it? We know because we trust. Not all of us have that luxury. Say you're a fugitive from the law. (Yes, I know, your favorite metaphor.) The authorities don't trust you much and—one can safely assume—the feeling is mutual. How then can you convince the police of your innocence? Reveal too little and they won't believe you. Reveal too much and they'll catch you. Intractability holds the key to the answer. And the Feds hold the key to my prison cell if I say more. Sorry, nothing to see here, move along.


Years have passed and you've traded your fugitive's garb for the funky duds of a math genius who's discovered how to factor integers in a flash. Sniffing a business opportunity, you offer to factor anybody's favorite number for a small fee. There might be a huge market for that, but it's less clear there's nearly enough gullibility around for anyone to take you up on your offer—especially with your mugshot still hanging in the post office. No one is likely to cough up any cash unless they can see the prime factors. But then why would you reward such distrust by revealing the factors in the first place? Obviously, some confidence-building is in order.
What will do the trick is a dialogue between you and the buyer that persuades her that you know the factors, all the while leaking no information about them whatsoever. Amazingly, such an unlikely dialogue exists: for this and, in fact, for any of our Jurassics. Alice can convince you that she can split up your iPod library evenly without dropping the slightest hint about how to do it. (A technical aside: this requires a slightly stronger intractability assumption than P?NP.) Say hello to the great zero-knowledge (ZK) paradox: a congenital liar can convince a hardened skeptic that she knows something without revealing a thing about it. ZK dialogues leave no option but for liars to tell the truth and for doubting Thomases to believe. They render dishonesty irrelevant, for trusting comes naturally to a society where all liars get caught.
What's intractability got to do with it? Everything. If factoring were known to be tractable, the buyer would need no evidence that you could factor: she could just do it herself and ignore your services—bakers don't buy bread. At this point, the reader might have a nagging suspicion of defective logic: if factoring is so hard, then who's going to be the seller? Superman? In e-commerce applications, numbers to be factored are formed by multiplying huge primes together. In this way, the factors are known ahead of time to those privy to this process and live in intractability limboland for all others.


It gets better. Not only can two parties convince each other of their respective knowledge without leaking any of it; they can also reason about it. Two businessmen get stuck in an elevator. Naturally, a single thought runs through their minds: finding out who's the wealthier. Thanks to ZK theory, they'll be able to do so without revealing anything about their own worth (material worth, that is—the other kind is already in full view).
Feel the pain of two nuclear powers, Learsiland and Aidniland. Not being signatories to the Nuclear Non-Proliferation Treaty, only they know the exact size of their nuclear arsenals (at least one hopes they do). Computing theory would allow Learsiland to prove to Aidniland that it outnukes it without leaking any information about its deterrent's strength. The case of Nariland is more complex: it only wishes to demonstrate compliance with the NPT (which it's signed) without revealing any information about its nuclear facilities. While these questions are still open, they are right up ZK 's alley. Game theorists made quite a name for themselves in the Cold War by explaining why the aptly named MAD strategy of nuclear deterrence was not quite as mad as it sounded. Expect zero-knowledgists to take up equally daunting “doomsday” challenges in the years ahead. And, when they do, get yourself a large supply of milk and cookies, a copy of Kierkegaard's “Fear and Trembling,” and unrestricted access to a deep cave.
More amazing than ZK still is this thing called PCP (for “probabilistically checkable proofs; not for what you think). For a taste of it, consider the sociological oddity that great unsolved math problems seem to attract crackpots like flypaper. Say I am one of them. One day I call the folks over at the Clay Math Institute to inform them that I've just cracked the Riemann hypothesis (the biggest baddest beast in the math jungle). And could they please deposit my million-dollar check into my Nigerian account presto? Being the gracious sort, Landon and Lavinia Clay indulge me with a comforting “Sure,” while adding the perfunctory plea: “As you know, we're a little fussy about the format of our math proofs. So please make sure yours conforms to our standards—instructions available on our web site, blah, blah.” To my relief, that proves quite easy—even with that damn caps lock key stuck in the down position—and the new proof is barely longer than the old one. Over at Clay headquarters, meanwhile, no one has any illusions about me (fools!) but, bless the lawyers, they're obligated to verify the validity of my proof.


To do that, they've figured out an amazing way, the PCP way. It goes like this: Mr and Mrs Clay will pick four characters from my proof at random and throw the rest in the garbage without even looking at it. They will then assemble the characters into a four-letter word and read it out loud very slowly—it's not broadcast on American TV, so it's okay. Finally, based on that word alone, they will declare my proof valid or bogus. The kicker: their conclusion will be correct! Granted, there's a tiny chance of error due to the use of random numbers, but by repeating this little game a few times they can make a screwup less likely than having their favorite baboon type all of Hamlet in perfect Mandarin.
At this point, no doubt you're wondering whether to believe this mumbo-jumbo might require not only applying PCP but also smoking it. If my proof is correct, I can see how running it through the Clays' gauntlet of checks and tests would leave it unscathed. But, based on a lonely four-letter word, how will they know I've cracked Riemann's hypothesis and not a baby cousin, like the Riemann hypothesis for function fields, or a baby cousin's baby cousin like 1+1=2? If my proof is bogus (perish the thought) then their task seems equally hopeless. Presumably, the formatting instructions are meant to smear any bug across the proof so as to corrupt any four letters picked at random. But how can they be sure that, in order to evade their dragnet, I haven't played fast and loose with their silly formatting rules? Crackpots armed with all-caps keyboards will do the darndest thing. Poor Mr and Mrs Clay! They must check not only my math but also my abidance by the rules. So many ways to cheat, so few things to check.


PCP is the ultimate lie-busting device. Why ultimate? Because it is instantaneous and foolproof. The time-honored approach to truth finding is the court trial, where endless questioning between two parties, each one with good reasons to lie, leads to the truth or to a mistrial, but never to an erroneous judgment (yes, I know). PCP introduces the instant-trial system. Once the case has been brought before the judge, it is decided on the spot after only a few seconds of cross-examination. Justice is fully served; and yet the judge will go back to her chamber utterly clueless as to what the case was about. PCP is one of the most amazing algorithms of our time. It steals philosophy's thunder by turning on its head basic notions of evidence, persuasion, and trust. Somewhere, somehow, Ludwig the Tractatus Man is smiling.
To say that we're nowhere near resolving P vs NP is a safe prophecy. But why? There are few mysteries in life that human stupidity cannot account for, but to blame the P=NP conundrum on the unbearable lightness of our addled brains would be a cop-out. Better to point the finger at the untamed power of the Algorithm—which, despite rumors to the contrary, was not named after Al Gore but after Abu ‘Abd Allah Muhammad ibn Musa al-Khwarizmi. As ZK and PCP demonstrate, tractability reaches far beyond the racetrack where computing competes for speed. It literally forces us to think differently. The agent of change is the ubiquitous Algorithm. Let's look over the horizon where its disruptive force beckons, shall we?



Thinking algorithmically

Algorithms are often compared to recipes. As clichés go, a little shopworn perhaps, but remember: no metaphor that appeals to one's stomach can be truly bad. Furthermore, the literary analogy is spot-on. Algorithms are—and should be understood as—works of literature. The simplest ones are short vignettes that loop through a trivial algebraic calculation to paint fractals, those complex, pointillistic pictures much in vogue in the sci-fi movie industry. Just a few lines long, these computing zingers will print the transcendental digits of p, sort huge sets of numbers, model dynamical systems, or tell you on which day of the week your 150th birthday will fall (something whose relevance we've already covered). Zingers can do everything. For the rest, we have, one notch up on the sophistication scale, the sonnets, ballads, and novellas of the algorithmic world. Hiding behind their drab acronyms, of which RSA, FFT, SVD, LLL, AKS, KMP, and SVM form but a small sample, these marvels of ingenuity are the engines driving the algorithmic revolution currently underway. (And, yes, you may be forgiven for thinking that a computer geek's idea of culinary heaven is a nice big bowl of alphabet soup.) At the rarefied end of the literary range, we find the lush, complex, multilayered novels. The Algorithmistas' pride and joy, they are the big, glorious tomes on the coffee table that everyone talks about but only the fearless read.


Give it to them, algorithmic zingers know how to make a scientist swoon. No one who's ever tried to calculate the digits of p by hand can remain unmoved at the sight of its decimal expansion flooding a computer screen like lava flowing down a volcano. Less impressive perhaps but just as useful is this deceptively simple data retrieval technique called binary search, or BS for short. Whenever you look up a friend's name in the phone book, chances are you're using a variant of BS—unless you're the patient type who prefers exhaustive search (ES) and finds joy in combing through the directory alphabetically till luck strikes. Binary search is exponentially (ie, incomparably) faster than ES. If someone told you to open the phone book in the middle and check whether the name is in the first or second half; then ordered you to repeat the same operation in the relevant half and go on like that until you spotted your friend's name, you would shoot back: “That's BS!”
Well, yes and no. Say your phone book had a million entries and each step took one second: BS would take only twenty seconds but ES would typically run for five days! Five days?! Imagine that. What if it were an emergency and you had to look up the number for 911? (Yep, there's no low to which this writer won't stoop.) The key to binary search is to have an ordered list. To appreciate the relevance of sorting, suppose that you forgot the name of your friend (okay, acquaintance) but you had her number. Since the phone numbers typically appear in quasi-random order, the name could just be anywhere and you'd be stuck with ES. There would be two ways for you to get around this: to be the famous Thomas Magnum and bribe the Honolulu police chief to get your hands on the reverse directory; or to use something called a hash table: a key idea of computer science.
Hash table? Hmm, I know what you're thinking: Algorithmistas dig hash tables; they're down for PCP; they crack codes; they get bent out of shape by morphin'; they swear by quicksnort (or whatever it's called). Coincidence? Computer scientists will say yes, but what else are they supposed to say?
Algorithms for searching the phone book or spewing out the digits of p are race horses: their sole function is to run fast and obey their masters. Breeding Triple Crown winners has been high on computer science's agenda—too high, some will say. Blame this on the sheer exhilaration of the sport. Algorithmic racing champs are creatures of dazzling beauty, and a chance to breed them is a rare privilege. That said, whizzing around the track at lightning speed is not the be-all and end-all of algorithmic life. Creating magic tricks is just as highly prized: remember RSA, PCP, ZK. The phenomenal rise of Google's fortunes owes to a single algorithmic gem, PageRank, leavened by the investing exuberance of legions of believers. To make sense of the World Wide Web is algorithmic in a qualitative sense. Speed is a secondary issue. And so PageRank, itself no slouch on the track, is treasured for its brains, not its legs.
Hold on! To make sense of the world, we have math. Who needs algorithms? It is beyond dispute that the dizzying success of 20th century science is, to a large degree, the triumph of mathematics. A page's worth of math formulas is enough to explain most of the physical phenomena around us: why things fly, fall, float, gravitate, radiate, blow up, etc. As Albert Einstein said, “The most incomprehensible thing about the universe is that it is comprehensible.” Granted, Einstein's assurance that something is comprehensible might not necessarily reassure everyone, but all would agree that the universe speaks in one tongue and one tongue only: mathematics.


But does it, really? This consensus is being challenged today. As young minds turn to the sciences of the new century with stars in their eyes, they're finding old math wanting. Biologists have by now a pretty good idea of what a cell looks like, but they've had trouble figuring out the magical equations that will explain what it does. How the brain works is a mystery (or sometimes, as in the case of our 43rd president, an overstatement) whose long, dark veil mathematics has failed to lift.
Economists are a refreshingly humble lot—quite a surprise really, considering how little they have to be humble about. Their unfailing predictions are rooted in the holy verities of higher math. True to form, they'll sheepishly admit that this sacred bond comes with the requisite assumption that economic agents, also known as humans, are benighted, robotic dodos—something which unfortunately is not always true, even among economists.
A consensus is emerging that, this time around, throwing more differential equations at the problems won't cut it. Mathematics shines in domains replete with symmetry, regularity, periodicity—things often missing in the life and social sciences. Contrast a crystal structure (grist for algebra's mill) with the World Wide Web (cannon fodder for algorithms). No math formula will ever model whole biological organisms, economies, ecologies, or large, live networks. Will the Algorithm come to the rescue? This is the next great hope. The algorithmic lens on science is full of promise—and pitfalls.
First, the promise. If you squint hard enough, a network of autonomous agents interacting together will begin to look like a giant distributed algorithm in action. Proteins respond to local stimuli to keep your heart pumping, your lungs breathing, and your eyes glued to this essay—how more algorithmic can anything get? The concomitance of local actions and reactions yielding large-scale effects is a characteristic trait of an algorithm. It would be naive to expect mere formulas like those governing the cycles of the moon to explain the cycles of the cell or of the stock market.
Contrarians will voice the objection that an algorithm is just a math formula in disguise, so what's the big hoopla about? The answer is: yes, so what? The issue here is not logical equivalence but expressibility. Technically, number theory is just a branch of set theory, but no one thinks like that because it's not helpful. Similarly, the algorithmic paradigm is not about what but how to think. The issue of expressiveness is subtle but crucial: it leads to the key notion of abstraction and is worth a few words here (and a few books elsewhere).
Remember the evil Brazilian butterfly? Yes, the one that idles the time away by casting typhoons upon China with the flap of a wing. This is the stuff of legend and tall tales (also known as chaos theory). Simple, zinger-like algorithms model this sort of phenomenon while neatly capturing one of the tenets of computing: the capacity of a local action to unleash colossal forces on a global scale; complexity emerging out of triviality.

Al-Khwarizmi takes wing
Create a virtual aviary of simulated geese and endow each bird with a handful of simple rules: (1) Spot a flock of geese? Follow its perceived center; (2) Get too close to a goose? Step aside; (3) Get your view blocked by another goose? Move laterally away from it; etc. Release a hundred of these critters into the (virtual) wild and watch a distributed algorithm come to life, as a flock of graceful geese migrate in perfect formation. Even trivial rules can produce self-organizing systems with patterns of behavior that look almost “intelligent.” Astonishingly, the simplest of algorithms mediate that sort of magic.
The local rules of trivial zingers carry enough punch to produce complex systems; in fact, by Church-Turing universality, to produce any complex system. Obviously, not even algorithmic sonnets, novellas, or Homeric epics can beat that. So why bother with the distinction? Perhaps for the same reason the snobs among us are loath to blur the difference between Jay Leno and Leo Tolstoy. But isn't “War and Peace” just an endless collection of one-liners? Not quite. The subtlety here is called abstraction. Train your binoculars on a single (virtual) goose in flight and you'll see a bird-brained, rule-driven robot flying over Dullsville airspace. Zoom out and you'll be treated to a majestic flock of birds flying in formation. Abstraction is the ability to choose the zoom factor. Algorithmic novels allow a plethora of abstraction levels that are entirely alien to zingers.
Take war, for example. At its most basic, war is a soldier valiantly following combat rules on the battlefield. At a higher level of abstraction, it is a clash of warfare strategies. Mindful of Wellington's dictum that Waterloo was won on the playing fields of Eton (where they take their pillow fighting seriously), one might concentrate instead on the schooling of the officer corps. Clausewitz devotees who see war as politics by other means will adjust the zoom lens to focus on the political landscape. Abstraction can be vertical: a young English infantryman within a platoon within a company within a battalion within a regiment within a mass grave on the banks of the Somme.
Or it can be horizontal: heterogeneous units interacting together within an algorithmic “ecology.” Unlike zingers, algorithmic novels are complex systems in and of themselves. Whereas most of what a zinger does contributes directly to its output, the epics of the algorithmic world devote most of their energies to servicing their constituent parts via swarms of intricate data structures. Most of these typically serve functions that bear no direct relevance to the algorithm's overall purpose—just as the mRNA of a computer programmer rarely concerns itself with the faster production of Java code.
The parallel with biological organisms is compelling but far from understood. To this day, for example, genetics remains the art of writing the captions for a giant cartoon strip. Molecular snapshots segue from one scene to the next through plots narrated by circuit-like chemical constructs—zingers, really—that embody only the most rudimentary notions of abstraction. Self-reference is associated mostly with self-replication. In the algorithmic world, by contrast, it is the engine powering the complex recursive designs that give abstraction its amazing richness: it is, in fact, the very essence of computing. Should even a fraction of that power be harnessed for modeling purposes in systems biology, neuroscience, economics, or behavioral ecology, there's no telling what might happen (admittedly, always a safe thing to say). To borrow the Kuhn cliché, algorithmic thinking could well cause a paradigm shift. Whether the paradigm shifts, shuffles, sashays, or boogies its way into the sciences, it seems destined to make a lasting imprint.


Now the pitfalls. What could disrupt the rosy scenario we so joyfully scripted? The future of the Algorithm as a modeling device is not in doubt. For its revolutionary impact to be felt in full, however, something else needs to happen. Let's try a thought experiment, shall we? You're the unreconstructed Algorithm skeptic. Fresh from splitting your playlist, Alice, naturally, is the advocate. One day, she comes to you with a twinkle in her eye and a question on her mind: “What are the benefits of the central law of mechanics?” After a quick trip to Wikipedia to reactivate your high school physics neurons and dust off the cobwebs around them, you reply that F=ma does a decent job of modeling the motion of an apple as it is about to crash on Newton's head: “What's not to like about that?” “Oh, nothing,” retorts Alice, “except that algorithms can be faithful modelers, too; they're great for conducting simulations and making predictions.” Pouncing for the kill, she adds: “By the way, to be of any use, your vaunted formulas will first need to be converted into algorithms.” Touché.
Ahead on points, Alice's position will dramatically unravel the minute you remind her that F=malives in the world of calculus, which means that the full power of analysis and algebra can be brought to bear. From F=ma, for example, one finds that: (i) the force doubles when the mass does; (ii) courtesy of the law of gravity, the apple's position is a quadratic function of time; (iii) the invariance of Maxwell's equations under constant motion kills F=ma and begets the theory of special relativity. And all of this is done with math alone! Wish Alice good luck trying to get her beloved algorithms to pull that kind of stunt. Math gives us the tools for doing physics; more important, it gives us the tools for doing math. We get not only the equations but also the tools for modifying, combining, harmonizing, generalizing them; in short, for reasoning about them. We get the characters of the drama as well as the whole script!
Is there any hope for a “calculus” of algorithms that would enable us to knead them like Play-Doh to form new algorithmic shapes from old ones? Algebraic geometry tells us what happens when we throw in a bunch of polynomial equations together. What theory will tell us what happens when we throw in a bunch of algorithms together? As long as they remain isolated, free-floating creatures, hatched on individual whims for the sole purpose of dispatching the next quacking duck flailing in the open-problems covey, algorithms will be parts without a whole; and the promise of the Algorithm will remain a promise deferred.
While the magic of algorithms has long held computing theorists in its thrall, their potential power has been chronically underestimated; it's been the life story of the field, in fact, that they are found to do one day what no one thought them capable of doing the day before. If proving limitations on algorithms has been so hard, maybe it's because they can do so much. Algorithmistas will likely need their own “Google Earth” to navigate the treacherous canyons of Turingstan and find their way to the lush oases amid the wilderness. But mark my words: the algorithmic land will prove as fertile as the one the Pilgrims found in New England and its settlement as revolutionary.
Truth be told, the 1776 of computing is not quite upon us. If the Algorithm is the New World, we are still building the landing dock at Plymouth Rock. Until we chart out the vast expanses of the algorithmic frontier, the P vs NP mystery is likely to remain just that. Only when the Algorithm becomes not just a body but a way of thinking, the young sciences of the new century will cease to be the hapless nails that the hammer of old math keeps hitting with maniacal glee.
One thing is certain. Moore's Law has put computing on the map: the Algorithm will now unleash its true potential. That's one prediction Lord Kelvin never made, so you may safely trust the future to be kind to it.
Algorithms play an increasingly important role in selecting what information is considered most
relevant to us, a crucial feature of our participation in public life. Search engines help us navigate
massive databases of information, or the entire web. Recommendation algorithms map our
preferences against others, suggesting new or forgotten bits of culture for us to encounter.
Algorithms manage our interactions on social networking sites, highlighting the news of one
friend while excluding another's. Algorithms designed to calculate what is "hot" or "trending" or
"most discussed" skim the cream from the seemingly boundless chatter that's on offer. Together,
these algorithms not only help us find information, they provide a means to know what there is to
know and how to know it, to participate in social and political discourse, and to familiarize
ourselves with the publics in which we participate. They are now a key logic governing the flows
of information on which we depend, with the "power to enable and assign meaningfulness,
managing how information is perceived by users, the 'distribution of the sensible.'" (Langlois
2012)
Algorithms need not be software: in the broadest sense, they are encoded procedures for
transforming input data into a desired output, based on specified calculations. The procedures
name both a problem and the steps by which it should be solved. Instructions for navigation may
be considered an algorithm, or the mathematical formulas required to predict the movement of a
celestial body across the sky. "Algorithms do things, and their syntax embodies a command
structure to enable this to happen" (Goffey 2008, 17). We might think of computers, then,
fundamentally as algorithm machines -- designed to store and read data, apply mathematical
procedures to it in a controlled fashion, and offer new information as the output. But these are
procedures that could conceivably be done by hand -- and in fact were (Light 1999).
But as we have embraced computational tools as our primary media of expression, and
have made not just mathematics but all information digital, we are subjecting human discourse 
and knowledge to these procedural logics that undergird all computation. And there are specific
implications when we use algorithms to select what is most relevant from a corpus of data
composed of traces of our activities, preferences, and expressions.
These algorithms, which I'll call public relevance algorithms, are -- by the very same
mathematical procedures -- producing and certifying knowledge. The algorithmic assessment of
information, then, represents a particular knowledge logic, one built on specific presumptions
about what knowledge is and how one should identify its most relevant components. That we
are now turning to algorithms to identify what we need to know is as momentous as having
relied on credentialed experts, the scientific method, common sense, or the word of God.
What we need is an interrogation of algorithms as a key feature of our information
ecosystem (Anderson 2011), and of the cultural forms emerging in their shadows (Striphas
2010), with a close attention to where and in what ways the introduction of algorithms into
human knowledge practices may have political ramifications. This essay is a conceptual map to
do just that. I will highlight six dimensions of public relevance algorithms that have political
valence:
1. Patterns of inclusion: the choices behind what makes it into an index in the first place,
what is excluded, and how data is made algorithm ready
2. Cycles of anticipation: the implications of algorithm providers' attempts to thoroughly
know and predict their users, and how the conclusions they draw can matter
3. The evaluation of relevance: the criteria by which algorithms determine what is relevant,
how those criteria are obscured from us, and how they enact political choices about
appropriate and legitimate knowledge
4. The promise of algorithmic objectivity: the way the technical character of the algorithm
is positioned as an assurance of impartiality, and how that claim is maintained in the
face of controversy
5. Entanglement with practice: how users reshape their practices to suit the algorithms they
depend on, and how they can turn algorithms into terrains for political contest,
sometimes even to interrogate the politics of the algorithm itself
6. The production of calculated publics: how the algorithmic presentation of publics back
to themselves shape a public's sense of itself, and who is best positioned to benefit from
that knowledge.
Considering how fast these technologies and the uses to which they are put are changing, this list
must be taken as provisional, not exhaustive. But as I see it, these are the most important lines of
inquiry into understanding algorithms as emerging tools of public knowledge and discourse.
It would also be seductively easy to get this wrong. In attempting to say something of
substance about the way algorithms are shifting our public discourse, we must firmly resist
putting the technology in the explanatory driver's seat. While recent sociological study of the
Internet has labored to undo the simplistic technological determinism that plagued earlier work,
that determinism remains an alluring analytical stance. A sociological analysis must not conceive
of algorithms as abstract, technical achievements, but must unpack the warm human and
institutional choices that lie behind these cold mechanisms. I suspect that a more fruitful
approach will turn as much to the sociology of knowledge as to the sociology of technology -- to
see how these tools are called into being by, enlisted as part of, and negotiated around collective
efforts to know and be known. This might help reveal that the seemingly solid algorithm is in
fact a fragile accomplishment. It also should remind us that algorithms are now a communication
technology; like broadcasting and publishing technologies, they are now "the scientific
instruments of a society at large," (Gitelman 2006, 5) and are caught up in and are influencing
the ways in which we ratify knowledge for civic life, but in ways that are more "protocological"
(Galloway 2004), i.e. organized computationally, than any medium before.
Patterns of Inclusion
Algorithms are inert, meaningless machines until paired with databases upon which to function.
A sociological inquiry into an algorithm must always grapple with the databases to which it is
wedded; failing to do so would be akin to studying what was said at a public protest, while
failing to notice that some speakers had been stopped at the park gates.
For users, algorithms and databases are conceptually conjoined: users typically treat them
as a single, working apparatus. And in the eyes of the market, the creators of the database and the
providers of the algorithm are often one and the same, or are working in economic and often
ideological concert. "Together, data structures and algorithms are two halves of the ontology of
the world according to a computer." (Manovich 1999, 84). Nevertheless, we can treat the two as
analytically distinct: before results can be algorithmically provided, information must be
collected, readied for the algorithm, and sometimes excluded or demoted.
Collection
We live in a historical moment in which, more than ever before, nearly all public activity
includes keeping copious records, cataloging activity, and archiving documents -- and we do
more and more of it on a communication network designed such that every login, every page
view, and every click leaves a digital trace. Turning such traces into databases involves a
complex array of information practices (Stalder and Mayer 2009): Google, for example, crawls
the web indexing websites and their metadata. It digitizes real world information, from library
collections to satellite images to comprehensive photo records of city streets. It invites users to
provide personal and social details as part of their Google+ profile. It keeps exhaustive logs of
every search query entered and every result clicked. It adds local information based on each
user's computer's data. It stores the traces of web surfing practices gathered through their massive
advertising networks.
Understanding what is included in such databases requires an attention to the collection
policies of information services, but should also extend beyond to the actual practices involved.
This is not just to spot cases of malfeasance, though there are some, but to understand how an
information provider thinks about the data collection it undertakes. The political resistance to
Google's StreetView project in Germany and India reminds us that the answer to the question,
"What does this street corner look like?" has different implications for those who want to go
there, those who live there, and those who believe that the answer should not be available in such
a public way. But it also reveals what Google thinks of as "public," an interpretation that is being
widely deployed across their service.
Readied for the algorithm
"Raw data is an oxymoron" (Gitelman and Jackson forthcoming). Data is both already
desiccated and remains messy. Nevertheless, there is a premeditated order necessary for
algorithms to even work. More than anything, algorithms are designed to be and prized for being
functionally automatic, to act when triggered without any regular human intervention or
oversight (Winner 1978). This means that the information included in the database must be
rendered into data, formalized so that algorithms can act on it automatically. Data must be
"imagined and enunciated against the seamlessness of phenomena" (Gitelman and Jackson
forthcoming). Recognizing the ways in which data must be "cleaned up" is an important counter
to the seeming automaticity of algorithms. Just as one can know something about sculptures 
from studying their inverted molds, algorithms can be understood by looking closely at how
information must be oriented to face them, how it is made algorithm-ready.
In the earliest database architectures, information was organized in strict and, as it turned
out, inflexible hierarchies. Since the development of relational and object-oriented database
architectures, information can be organized in more flexible ways, where bits of data can have
multiple associations with other bits of data, categories can change over time, and data can be
explored without having to navigate or even understand the hierarchical structure by which it is
archived. The sociological implications of database design has largely been overlooked; the
genres of databases themselves have inscribed politics, as well as making algorithms essential
information tools. As Rieder (2012) notes, with the widespread uptake of relational databases
comes a "relational ontology" that understands data as atomized, "regular, uniform, and only
loosely connected objects that can be ordered in a potentially unlimited number of ways at the
time of retrieval," thereby shifting expressive power from the structural design of the database to
the query.
Even with these more flexible forms of databases, categorization remains vitally
important to database design and management. Categorization is a powerful semantic and
political intervention: what the categories are, what belongs in a category, and who decides how
to implement these categories in practice, are all powerful assertions about how things are and
are supposed to be (Bowker and Star 2000). Once instituted, a category draws a demarcation
that will be treated with reverence by an approaching algorithm. A useful example here is the
#amazonfail incident. In 2009, more than fifty-seven thousand gay-friendly books disappeared in
an instant from Amazon's sales lists, because they had been accidentally categorized as "adult."
Naturally, complex information systems are prone to error. But this particular error also revealed
that Amazon's algorithm calculating "sales rank" is instructed to ignore books designated as
adult. Even when mistakes are not made, whatever criteria Amazon uses to determine adult-ness
are being applied and reified -- apparent only in the unexplained absence of some books and the
presence of others.
Exclusion and demotion
Though all database producers share an appetite for gathering information, they are made
distinctive more by what they choose to exclude. "The archive, by remembering all and only a
certain set of facts / discoveries / observations, consistently and actively engages in the forgetting 
of other sets … The archive's jussive force, then, operates through being invisibly exclusionary.
The invisibility is an important feature here: the archive presents itself as being the set of all
possible statements, rather than the law of what can be said." (Bowker 2008, 12-14) Even in the
current conditions of digital abundance (Keane 1999), in which it is cheaper and easier to err on
the side of keeping information rather than not, there is always a remainder.
Sites can, themselves, refuse to allow data collectors (like search engines) to index their
sites. Elmer (2008) reveals that robot.txt, a bit of code that prevents search engines from
indexing a page or site, though designed initially as a tool for preserving the privacy of
individual creators, has since been used by government institutions to "redact" otherwise public
documents from public scrutiny. But beyond self-exclusion, some information initially collected
is subsequently removed before an algorithm ever gets to it. Though large-scale information
services pride themselves on being comprehensive, these sites are and always must be censors as
well. Indexes are culled of spam and viruses, patrolled for copyright infringement and
pornography, and scrubbed of the obscene, the objectionable, or the politically contentious
(Gillespie forthcoming).
Offending content can simply be removed from the index, or an account suspended,
before it ever reaches another user. But, in tandem with an algorithm, problematic content can be
handled in more subtle ways. YouTube "algorithmically demotes" suggestive videos, so they do
not appear on lists of the most watched, or on the home page generated for new users. Twitter
does not censor profanity from public tweets, but it does remove it from their algorithmic
evaluation of which terms are Trending.
The particular patterns whereby information is either excluded from a database, or
included and then managed in particular ways, are reminiscent of 20th century debates (Tushnet
2008) about the ways choices made by commercial media about who is systematically left out
and what categories of speech simply don't qualify can shape the diversity and character of
public discourse. Whether enacted by a newspaper editor or by a search engine's indexing tools,
these choices help establish and confirm standards of viable debate, legitimacy, and decorum.
But here, the algorithms can be touted as automatic, while it is the patterns of inclusion that
predetermine what will or will not appear among their results.
Cycles of Anticipation
Search algorithms determine what to serve up based on input from the user. But most platforms
now make it their business to know much, much more about the user than the query they just
entered. Sites hope to anticipate the user at the moment the algorithm is called upon, which
requires knowledge of that user gleaned at that instant, knowledge of that user already gathered,
and knowledge of users estimated to be statistically and demographically like them (Beer 2009) -
-drawing together what Stalder and Mayer (2009) call the "second index." If broadcasters were
providing not just content to audiences but also audiences to advertisers (Smythe 2001), digital
providers are not just providing information to users, they are also providing users to their
algorithms. And algorithms are made and remade in every instance of their use because every
click, every query, changes the tool incrementally.
Much of the scholarship about the data collection and tracking practices of contemporary
information providers has focused on the significant privacy concerns they provoke. Zimmer
(2008) argues that search engines now aspire to not only relentlessly index the web but also to
develop "perfect recall" of all of their users. To do this, information providers must not just track
their users, they must also build technical infrastructures and business models that link individual
sites into a suite of services (like Google's many tools and services) or an even broader
ecosystem (as with Facebook's "social graph" and its "like" buttons scattered across the web),
and then create incentives for users to remain within it. This allows the provider to be "passiveaggressive"
(Berry 2012) in how it assembles information gathered across many sites into a
coherent and increasingly comprehensive profile. Providers also take advantage of the
increasingly participatory ethos of the web, where users are powerfully encouraged to volunteer
all sorts of information about themselves, and encouraged to feel powerful doing so. As our
micro-practices migrate more and more to these platforms, it is seductive (though not obligatory)
for information providers to both track and commodify that activity in a variety of ways
(Gillespie and Postigo 2012). Moreover, users may be unaware that their activity across the web
is being tracked by the biggest online advertisers, and they are in little position to challenge this
arrangement even if they do (Turow 2012).
Yet privacy is not the only politically relevant concern. In these cycles of anticipation, it
is the bits of information that are most legible to the algorithm, and thus tend to stand in for those
users. What Facebook knows about its users is a great deal; but still, it knows only what it is able 
to know. The most knowable information (geo-location, computing platform, profile
information, friends, status updates, links followed on the site, time on the site, activity on other
sites that host "like" buttons or cookies) is a rendering of that user, a "digital dossier" (Solove
2004) or "algorithmic identity" (Cheney-Lippold 2011) that is imperfect but sufficient. What is
less legible or cannot be known about users falls away or is bluntly approximated. As Balka
(2011) described it, information systems produce "shadow bodies" by emphasizing some aspects
of their subjects and overlooking others. These shadow bodies persist and proliferate through
information systems, and the slippage between the anticipated user and the user themselves that
they represent can be either politically problematic, or politically productive.
But algorithms are not always about exhaustive prediction; sometimes they are about
sufficient approximation. Perhaps just as important as the surveillance of users are the
conclusions providers are willing to draw based on relatively little information about them.
Hunch.com, a content recommendation service, boasted that they could know a user's
preferences with 80-85% accuracy based on the answers to just five questions. While this
radically boils down the complexity of a person to five points on a graph, what's important is that
this is sufficient accuracy for their purposes (Zuckerman 2011). Because such sites are
comfortable catering to these user-caricatures, the questions that appear to sort us most
sufficiently, particularly around our consumer preferences, are likely to grow in significance as
public measures. And to some degree, we are invited to formalize ourselves into these knowable
categories. When we encounter these providers, we are encouraged to choose from the menus
they offer, so as to be correctly anticipated by the system and provided the right information, the
right recommendations, the right people.
Beyond knowing the personal and the demographic details about each user, information
providers conduct a great deal of research trying to understand, and then operationalize, how
humans habitually seek, engage with, and digest information. Most notably in the study of
human-computer interaction (HCI), the understanding of human psychology and perception is
brought to bear on the design of algorithms and the ways in which their results should be
represented. Designers hope to anticipate users' psycho-physiological capabilities and tendencies,
not just specific users' preferences and habits. But in these anticipations, too, implicit and
sometimes political valences can be inscribed in the technology (Sterne 2008): the perceptual or
interpretive habits of some users are taken to be universal, contemporary habits are imagined to
be timeless, particular computational goals are assumed to be self-evident.
We are also witnessing a new kind of information power, gathered in these enormous
databases of user activity and preference, which is itself reshaping the political landscape.
Regardless of their techniques, information providers who amass this data, third party industries
who gather and purchase user data as a commodity for them, and those who traffic in user data
for other reasons (that is, credit card companies), have a stronger voice because of it, in both the
marketplace and in the halls of legislative power, and are increasingly involving themselves in
political debates about consumer safeguards and digital rights. We are seeing the deployment of
data mining in the arenas of political organizing (Howard 2005), journalism (Anderson 2011),
and publishing (Striphas 2009), where the secrets drawn from massive amounts of user data are
taken as compelling guidelines for future content production, be it the next micro-targeted
campaign ad or the next pop phenomenon.
The Evaluation of Relevance
When users click "Search," or load their Facebook News Feed, or ask for recommendations from
Netflix, algorithms must instantly and automatically identify which of the trillions of bits of
information best meets the criteria at hand, and will best satisfy a specific user and their
presumed aims. While these calculations have never been simple, they have grown more
complex as the public use of these services has matured. Search algorithms, for example, once
based on simply tallying how often the actual search terms appear in the indexed web pages, now
incorporate contextual information about the sites and their hosts, consider how often the site is
linked to by others and in what way, and enlist natural language processing techniques to better
"understand" both the query and the resources that the algorithm might return in
response. According to Google, its search algorithm examines over 200 signals for every query.1
These signals are the means by which the algorithm approximates "relevance.” But here
is where sociologists of algorithms must firmly plant their feet: "relevant" is a fluid and loaded
judgment, as open to interpretation as some of the evaluative terms media scholars have already
unpacked, like “newsworthy” or “popular.” As there is no independent metric for what actually
are the most relevant search results for any given query, engineers must decide what results look
"right" and tweak their algorithm to attain that result, or make changes based on evidence from
their users, treating quick clicks and no follow-up searches as an approximation, not of relevance
exactly, but of satisfaction. To accuse an algorithm of bias implies that there exists an unbiased 
judgment of relevance available, to which the tool is failing to hew. Since no such measure is
available, disputes over algorithmic evaluations have no solid ground upon which to fall back.
Criteria
To be able to say that a particular algorithm makes evaluative assumptions, the kind that
have consequences for human knowledge endeavors, might call for a critical analysis of the
algorithm to interrogate its underlying criteria. But in nearly all cases, such evaluative criteria are
hidden, and must remain so. Twitter's Trends algorithm, which reports to the user what terms are
"trending" at that moment in their area, even leaves the definition of "trending" unspecified. The
criteria they use to assess 'trendiness' are only described in general terms: the velocity of a
certain term's surge, whether it has appeared in the Trend list before, whether it circulates within
or spans across clusters of users. What is unstated is how these criteria are measured, how they
are weighed against one another, what other criteria have also been incorporated, and when if
ever these criteria will be overridden. This leaves algorithms perennially open to user suspicion
that their criteria skew to the provider’s commercial or political benefit, or incorporate
embedded, unexamined assumptions that act below the level of awareness, even that of the
designers (Gillespie 2012).
An information provider like Twitter cannot be much more explicit or precise about its
algorithm’s workings. To do so would give competitors an easy means of duplicating and
surpassing their service. It would also require a more technical explanation than most users are
prepared for. It would hamper their ability to change their criteria as they need. But most of all, it
would hand those who hope to "game the system" a road map for getting their sites to the top of
the search results or their hashtags to appear on the Trends list. While some collaborative
recommendation sites like Reddit have made public their algorithms for ranking stories and user
comments, these sites must constantly seek out and correct instances of organized downvoting,
and these tactics cannot be made public. With a few exceptions, the tendency is strongly toward
being oblique.2
Commercial aims
A second approach might entail a careful consideration of the economic and the cultural contexts
from which the algorithm came. Any knowledge system emerges amidst the economic and
political aims of information provision, and will be shaped by the aims and strategies of those 
powerful institutions looking to capitalize on it (Hesmondhalgh 2006). The pressures faced by
search engines, content platforms, and information providers can subtly shape the design of the
algorithm itself and the presentation of its results (Vaidhyanathan 2011). As the algorithm comes
to stand as a legitimate knowledge logic, new commercial endeavors are fitted to it (for instance,
search engine optimization), reifying choices made and forcing additional ones.
For example, early critics worried that search engines would offer up advertisements in
the form of links or featured content, presented as the product of algorithmic calculations. The
rapid and clear public rejection of this ploy demonstrated how strong our trust in these
algorithms is: users did not wish the content that providers wanted us to see for financial reasons,
to be intermingled with content that the provider had algorithmically selected. But the concern is
now multidimensional: the landscape of the Facebook News Feed, for example, can no longer be
described as two distinct territories, social and commercial; rather, it interweaves the results of
algorithmic calculations (what status updates and other activities of friends should be listed in the
Feed, what links will be recommended to this user, which friends are actively on the site at the
moment), structural elements (tools for contributing a status update, commenting on an
information element, links to groups and pages), and elements placed there based on a
sponsorship relationship (banner ads, apps from third party sites). To map this complex terrain
requires a deep understanding of the economic relationships and social assumptions it represents.
Epistemological premises
Finally, we must consider if the evaluative criteria of the algorithm are structured by specific
political or organizational principles that themselves have political ramifications. This is not just
whether an algorithm might be partial to this or that provider or might favor its own commercial
interests over others. It is a question of whether the philosophical presumptions about relevant
knowledge on which the algorithm is founded matters. Some early scholarship looking at the
biases of search engines (in order of publication, Introna and Nissenbaum 2000; Halavais 2008;
Rogers 2009; Granka 2010) noted some structural tendencies toward what's already popular,
toward English-speaking sites, and toward commercial information providers. Legal scholars
debating what it would mean to require neutrality in search results (Grimmelmann 2010;
Pasquale and Bracha 2008) have meant more than just the inability to tip results toward a
commercial partner.
The criteria public information algorithms take into account are myriad; each is fitted
with a threshold for what will push something up in the results, position one result above
another, and so on. So evaluations performed by algorithms always depend on inscribed
assumptions about what matters, and how what matters can be identified. When a primitive
search engine counted the number of appearances of a search term on the web pages it had
indexed, it reified a particular logic, one that assumed that pages that include the queried term
were likely to be relevant to someone interested in that term. When Google developed PageRank,
factoring in incoming links to a page as evidence of its value, it built in a different logic: a page
with many incoming links, from high-quality sites, is seen as "ratified" by other users, and is
more likely to be relevant to this user as well. By preferring incoming links from sites
themselves perceived to be of high-quality, Finkelstein notes, Google had shifted from a more
populist approach to a "shareholder democracy:" "One link is not one vote, but it has influence
proportional to the relative power (in terms of popularity) of the voter. Because blocks of
common interests, or social factions, can affect the results of a search to a degree depending on
their relative weight in the network, the results of the algorithmic calculation by a search engine
come to reflect political struggles in society" (Finkelstein 2008, 107). When a news discussion
site decides what ratio of negative complaints to number of views is sufficient to justify
automatically hiding a comment thread, it represents their assessment of the proper volatility of
public discourse, or at least the volatility they prefer, for the user community they think they
have (Braun 2011). A great deal of expertise and judgment can be embedded in these cognitive
artifacts (Hutchins 1995; Latour 1986), but it is judgment that is then submerged and automated.
Most users do not dwell on algorithmic criteria, tending to treat them as unproblematic
tools in the service of a larger activity: finding an answer, solving a problem, being entertained.
However, while the technology may be "black boxed" (Latour 1987; Pinch and Bijker, 1984) by
designers and users alike, that should not lead us to believe that it remains stable. In fact,
algorithms can be easily, instantly, radically, and invisibly changed. While major upgrades may
happen only on occasion, algorithms are regularly being "tweaked." Changes can occur without
the interface to the algorithm changing in the slightest: the Facebook news feed and search bar
may look the same as they did yesterday, while the evaluations going on beneath them have been
thoroughly remade. The black box metaphor fails us here, as the workings of the algorithm are
both obscured and malleable, "likely so dynamic that a snapshot of them would give us little
chance of assessing their biases" (Pasquale 2009). In fact, what we might refer to as an algorithm 
is often not one algorithm but many. Search engines like Google regularly engage in "A/B"
testing,3 presenting different rankings to different subsets of users to gain on-the-fly data on
speed and customer satisfaction, then incorporating the adjustments preferred by users in a
subsequent upgrade.
Each algorithm is premised on both an assumption about the proper assessment of
relevance, and an instantiation of that assumption into a technique for (computational)
evaluation. There may be implicit premises built into a site's idea of relevance, there may be
shortcuts built into its technical instantiation of that idea, and there may be friction between the
two.
The Promise of Algorithmic Objectivity
More than mere tools, algorithms are also stabilizers of trust, practical and symbolic assurances
that their evaluations are fair and accurate, free from subjectivity, error, or attempted influence.
But, though algorithms may appear to be automatic and untarnished by the interventions of their
providers, this is a carefully crafted fiction. "Search engines pride themselves on being
automated, except when they aren't." (Grimmelmann 2008, 950) In fact, no information service
can be completely hands-off in its delivery of information: though an algorithm may evaluate
any site as most relevant to your query, that result will not appear if it is child pornography, it
will not appear in China if it is dissident political speech, and it will not appear in France if it
promotes Nazism. Yet it's very important for the providers of these algorithms that they seem
hands-off. The legitimacy of these functioning mechanisms must be performed alongside the
provision of information itself.
The articulations offered by the algorithm provider alongside their tool are meant to
provide what Pfaffenberger (1992) calls "logonomic control," to define their tool within the
practices of users, to bestow the tool with a legitimacy that then carries to the information
provided and, by proxy, the provider. The careful articulation of an algorithm as impartial (even
when that characterization is more obfuscation than explanation) certifies it as a reliable sociotechnical
actor, lends its results relevance and credibility, and maintains the provider's apparent
neutrality in the face of the millions of evaluations it makes. This articulation of the algorithm is
just as crucial to its social life as its material design and its economic obligations.
It is largely up to the providers to describe their algorithm as being of a particular shape,
having therefore a certain set of values, and thus conferring to it some kind of legitimacy. This
includes carefully characterizing the tool and its value to a variety of audiences, sometimes in a
variety of ways: an algorithm can be defended as a tool for impartial evaluation to those critical
of its results, and at the same time be promised as a tool for selective promotion to potential
advertisers (Gillespie 2010). As Mackenzie (2005) observes, this process requires more than a
single, full-throated description: it depends both on "repetition and citation," (81) and at the same
time requires "the 'covering over' of the 'authoritative set of practices' that lend it force." (82)
When an information provider finds itself criticized for the results it provides, the legitimacy of
its algorithm must be repaired both discursively and technically. And users are complicit in this:
"A society that obsesses over the top Google News results has made those results important, and
we are ill-advised to assume the reverse (that the results are obsessed over because they are
important) without some narrative account of why the algorithm is superior to, say, the "news
judgment" of editors at traditional media" (Pasquale 2009).
This articulation happens first in the presentation of the tool, in its deployment within a
broader information service. Calling them "results" or "best" or "top stories" or "trends" speaks
not only to what the algorithm is actually measuring, but to what it should be understood as
measuring. An equally important part of this discursive work comes in the form of describing
how the algorithm works. Even what may seem like a clear explanation of a behind-the-scenes
process is always a "performed backstage" (Hilgartner 2000), carefully crafted to further
legitimize the process and its results. The description of Google's PageRank system, the earliest
component of its complex search algorithm, was published first as a technical paper (already a
crafted rendition of its mathematical workings), but was subsequently mythologized -- as the
defining feature of the tool, as the central element that made Google stand out above its then
competitors, and as a fundamentally democratic computational logic -- even as the algorithm was
being redesigned to take into account hundreds of other criteria.
Above all else, the providers of information algorithms must assert that their algorithm is
impartial. The performance of algorithmic objectivity has become fundamental to the
maintenance of these tools as legitimate brokers of relevant knowledge. No provider has been
more adamant about the neutrality of its algorithm than Google, which regularly responds to
requests to alter their search results with the assertion that the algorithm must not be tampered
with. Google famously pulled out of China in 2010 entirely rather than censor its results, though 
they had complied with China's rules before, and they may have pulled out rather than admitting
that they were losing to their Chinese competitors. Despite their stance, they did alter their search
results when complaints arose about a racist Photoshopped image of Michelle Obama at the top
of the Image search results; they provide a SafeSearch mechanism for keeping profanity and
sexual images from minors; and they refuse to autocomplete search queries that specify torrent
file-trading services. Yet Google regularly claims that it does not alter its index or manipulate its
results. Morozov (2011) believes that this is a way to deflect responsibility: "Google's spiritual
deferral to 'algorithmic neutrality' betrays the company's growing unease with being the world's
most important information gatekeeper. Its founders prefer to treat technology as an autonomous
and fully objective force rather than spending sleepless nights worrying about inherent biases in
how their systems -- systems that have grown so complex that no Google engineer fully
understands them -- operate."
This assertion of algorithmic objectivity plays in many ways an equivalent role to the
norm of objectivity in Western journalism. Like search engines, journalists have developed
tactics for determining what is most relevant, how to report it, and how to assure its relevance --
a set of practices that are relatively invisible to their audience, that they admit are messier to deal
with than they might appear, and that help set aside but do not eradicate value judgments and
personal politics. These institutionalized practices are animated by a conceptual promise that, in
the discourse of journalism, is regularly articulated (or overstated) as a kind of totem. Journalists
use the norm of objectivity as a "strategic ritual" (Tuchman 1972), to lend public legitimacy to
knowledge production tactics that are inherently precarious. "Establishing jurisdiction over the
ability to objectively parse reality is a claim to a special kind of authority" (Schudson and
Anderson 2009, 96).
Journalist and algorithmic objectivities are by no means the same. Journalistic objectivity
depends on an institutional promise of due diligence, built into and conveyed via a set of norms
journalists learned in training and on the job; their choices represent a careful expertise backed
by a deeply infused, philosophical and professional commitment to set aside their own biases and
political beliefs. The promise of the algorithm leans much less on institutional norms and trained
expertise, and more on a technologically-inflected promise of mechanical neutrality. Whatever
choices are made are presented both as distant from the intervention of human hands, and as
submerged inside of the cold workings of the machine.
But in both, legitimacy depends on accumulated guidelines for the proceduralization of
information selection. The discourses and practices of objectivity have come to serve as a
constitutive rule of journalism (Ryfe 2006). Objectivity is part of how journalists understand
themselves and what it means to be a journalist. It is part of how their work is evaluated, by
editors, colleagues, and by their readers. It is a defining signal by which journalists even
recognize what counts as journalism. The promise of algorithmic objectivity, too, has been
palpably incorporated into the working practices of algorithm providers, constitutively defining
the function and purpose of the information service. When Google includes in its "Ten Things
We Know to Be True" manifesto that "Our users trust our objectivity and no short-term gain
could ever justify breaching that trust," this is neither spin nor corporate Kool-Aid. It is a deeply
ingrained understanding of the public character of their information service, one that both
influences and legitimizes many of their technical and commercial undertakings, and helps
obscure the messier reality of the service they provide.
Still, these claims must compete in the public dialogue with other articulations, which
may or may not be so friendly to the economic and ideological aims of the stakeholders. Bijker
(1997) calls these competing "technological frames" the discursive characterizations of a
technology made by groups of actors who also have a stake in that technology's operation,
meaning, and social value. What users of an information algorithm take it to be, and whether
they are astute or ignorant, matters. How the press portrays such tools will strengthen or
undermine the providers' careful discursive efforts. This means that, while the algorithm itself
may seem to possess an aura of technological neutrality, or to embody populist, meritocratic
ideals, how it comes to appear that way depends not just on its design but also on the mundane
realities of news cycles, press releases, tech blogs, fan discussion, user rebellion, and the
machinations of their competitors.
There is a fundamental paradox in the articulation of algorithms. Algorithmic objectivity
is an important claim for a provider, particularly for algorithms that serve up vital and volatile
information for public consumption. Articulating the algorithm as a distinctly technical
intervention helps an information provider answer charges of bias, error, and manipulation. At
the same time, as can be seen with Google's PageRank, there is a sociopolitical value in
highlighting the populism of the criteria the algorithm uses. To claim that an algorithm is a
democratic proxy for the web-wide collective opinion of a particular website lends it authority.
And there is commercial value in claiming that the algorithm returns "better" results than its 
competitors, which posits customer satisfaction over some notion of accuracy (van Couvering
2007). In examining the articulation of an algorithm, we should pay particular attention to how
this tension between technically-assured neutrality and the social flavor of the assessment being
made is managed -- and, sometimes, where it breaks down.
Entanglement with Practice
Though they could be studied as abstract computational tools, algorithms are built to be
embedded into practice in the lived world that produces the information they process, and in the
lived world of their users (Couldry 2012). This is especially true when the algorithm is the
instrument of a business for whom the information it delivers (or the advertisements it pairs with
it) is the commodity. If users fail or refuse to fit that tool into their practices, to make it
meaningful, that algorithm will fail. This means we must consider not their "effect" on people,
but a multidimensional "entanglement" between algorithms put into practice and the social
tactics of users who take them up. This relationship is, of course, a moving target, because
algorithms change, and the user populations and activities they encounter change as well. Still,
this should not imply that there is no relationship. As these algorithms have nestled into people's
daily lives and mundane information practices, users shape and re-articulate the algorithms they
encounter; and algorithms impinge on how people seek information, how they perceive and think
about the contours of knowledge, and how they understand themselves in and through public
discourse.
It is important that we conceive of this entanglement not as a one-directional influence,
but as a recursive loop between the calculations of the algorithm and the "calculations" of
people. The algorithm that helps users navigate Flickr's photo archive is built upon the archive of
photos posted, which means it is designed to apprehend and reflect back the choices made by
photographers. What people do and do not photograph is already a kind of calculation, though
one that is historical, multivalent, contingent, and sociologically informed. But these were not
Flickr's only design impulses; sensitivity to photographic practices had to compete with cost,
technical efficiency, legal obligation, and business imperatives. And the population of Flickr
users and the types of photos they post changed as the site grew in popularity, was forced to
compete with Facebook, introduced tiered pricing, was bought by Yahoo, and so forth.
Many Flickr users post photos with the express purpose of having them be seen: some are
professional photographers looking for employment, some are seeking communities of likeminded
hobbyists, some are simply proud of their work. So just as the algorithm must be
sensitive to photographers, photographers have an interest in being sensitive to the algorithm,
aware that being delivered in response to the right search might put their photo in front of the
right people. Just as Hollywood's emphasis on specific genres invites screenwriters to write in
generic ways,4 the Flickr algorithm may induce subtle reorientations of photographers' practices
toward its own constructed logic, that is, toward aspiring to photograph in ways adherent to
certain emergent categories, or orienting their choice of subject and composition toward those
things the algorithm appears to privilege. "What we leave traces of is not the way we were, but a
tacit negotiation between ourselves and our imagined auditors" (Bowker 2009, 6-7).
Algorithmically recognizable
This tacit negotiation consists first and foremost of the mundane, strategic reorientation
of practices many users undertake, toward a tool that they know could amplify their efforts.
There is a powerful and understandable impulse for producers of information to make their
content, and themselves, recognizable to an algorithm. A whole industry, search engine
optimization (SEO), promises to boost websites to the top of search results. But we might think
of optimization (deliberate, professional) as just the leading edge of a much more varied,
organic, and complex process by which content producers of all sorts orient themselves toward
algorithms. When we use hashtags in our tweets -- a user innovation that was embraced later by
Twitter -- we are not just joining a conversation or hoping to be read by others, we are redesigning
our expression so as to be better recognized and distributed by Twitter's search
algorithm. Some may work to be noticed by the algorithm: teens have been known to tag their
status updates with unrelated brand names, in the hopes that Facebook will privilege those
updates in their friends' feeds.5 Others may work to evade an algorithm: Napster and P2P users
sharing infringing copyrighted music were known to slightly misspell the artists' names, so users
might find "Britny Speers" recordings but the record industry software would not.6
Is this gaming the system? Or is it a fundamental way we, to some degree, orient
ourselves toward the means of distribution through which we hope to speak? Based on the
criteria of the algorithm in question (or by our best estimate of its workings), we make ourselves
already algorithmically recognizable in all sorts of ways. This is not so different than 
newsmakers orienting their efforts to fit the routines of the news industry: timing a press release
to make the evening broadcast, or providing packaged video to a cable outlet hungry for gripping
footage, are techniques for turning to face the medium that may amplify them. Now, for all of us,
social networks and the web offer some analogous kind of "mediated visibility" (Thompson
2005, 49), and we gain similar benefit by turning to face these algorithms.
Backstage access
But who is best positioned to understand and operate the public algorithms that matter so much
to the public circulation of knowledge? Insight into the workings of information algorithms is a
form of power: vital to participating in public discourse, essential to achieving visibility online,
constitutive of credibility and the opportunities that follow. As mentioned before, the criteria and
code of algorithms are generally obscured -- but not equally or from everyone. For most users,
their understanding of these algorithms may be vague, simplistic, sometimes mistaken; they may
attempt to nudge the algorithm in ways that are either simply considered best practices (hashtags,
metadata) or that fundamentally misunderstand the algorithm's criteria (as with repeatedly
retweeting the same message in the hopes of Trending on Twitter). Search engine optimizers and
spammers have just as little access, but have developed a great deal of technical skill in divining
the criteria beneath the algorithm through testing and reverse engineering. Communities of
technology enthusiasts and critics engage in similar attempts to uncover the workings of these
systems, whether for fun, insight, personal advantage, or determined disruption. Legislators, who
have only just begun to ask questions about the implications of algorithms for fair commerce or
political discourse, have thus far been given only the most general of explanations: information
providers often contend that their algorithms are trade secrets that must not be divulged in a
public venue.
On the other hand, some stakeholders are granted access to the algorithm, though under
controlled conditions. Advertisers are offered one kind of access to the backstage workings of
that system, for bidding on preferred placement. Information providers that offer APIs to their
commercial partners and third party developers give them a glimpse under the hood, but bind
them with contracts and nondisclosure agreements in the very same moment. Access to,
understanding of, and rights regarding the algorithms that play a crucial role in public discourse
and knowledge will likely change, for different stakeholders and under specific circumstances --
changing also the power to build for, navigate through, and regulate these algorithms available to
these stakeholders and those they represent.
Domestication
As much as these tools may urge us to make ourselves legible to them, we also take them
into our practices, shifting their meaning and sometimes even their design along the way.
Silverstone (1994) has suggested that once technologies are offered to the public, they undergo a
process of "domestication:" literally, these technologies enter the home, but also figuratively,
users make them their own, embedding them in their routines, imbuing them with additional
meanings that the technology provider could not have anticipated. Public information algorithms
certainly matter for the way each user finds information, communicates with others, and knows
the world around them. But more than that, users express preferences for their favorite search
engines, opine about a site's recommendations as being buggy or intuitive or spot on. Some users
put great stock in a particular tool, while others come to distrust it, using it warily or not at all.
Apple iPhone users swap tips on how to make its Siri search agent speak its repertoire of
amusing retorts,7 then share in the outrage about its answers on hot button political issues.8
Satisfied Facebook users today become critics tomorrow when the algorithm behind their news
feed is altered in a way that feels economically motivated -- while through and after the uprising,
they continue to post status updates. Users, faced with the power asymmetries of data collection
and online surveillance, have developed an array of tactics of "obfuscation" to evade or pollute
the algorithmic attempts to know them (Brunton and Nissenbaum 2011). While it is crucial to
consider the ways algorithmic tools shape our encounters with information, we should not imply
that users are under the sway of these tools. The reality is more complicated, and more intimate.
Users can also turn to these algorithms for a data-inflected reflection; many sites allow us
to present ourselves to others and back to ourselves, including our public profile, the
performance of our friendships, the expression of our preferences, or a record of our recent
activity. Facebook's Timeline curates users' activities into chronological remembrances of them;
the pleasure of seeing what it algorithmically selects offers a kind of delight, a delight beyond
composing the photos and news posts in the first place. But algorithms can also function as a
particularly compelling "technology of the self" (Foucault 1988) when they seem to
independently ratify one's public visibility. It is now common practice to Google oneself: seeing
me appear as the top result in a search for my name offers a kind of assurance of my tenuous 
public existence. There is a sense of validation when your pet topic Trends on Twitter, when
Amazon recommends a book you already love, or when Apple iTunes' "Genius" function
composes an appealing playlist from your library of songs. Whether we actually tailor our
Amazon purchases so as to appear well-read (just as Nielsen ratings families used to over-report
watching PBS and C-Span) or we simply enjoy when the algorithm confirms our sense of self,
algorithms are a powerful invitation to understand ourselves through the independent lens they
promise to provide.
Algorithms are not just what designers make of them, or what they make of the
information they process. They are also what we make of them day in and day out -- but with this
caveat: because the logic, maintenance, and redesign of these algorithms remain in the hands of
the information providers, they are in a distinctly privileged position to rewrite our understanding
of them, or to engender a lingering uncertainty about their criteria that makes it difficult for us to
treat them as truly our own.
Knowledge logics
It is easy to theorize, but substantially more difficult to document, how users may shift their
worldviews to accommodate the underlying logics and implicit presumptions of the algorithms
they use regularly. There is a case to be made that the working logics of these algorithms not
only shape user practices, but lead users to internalize their norms and priorities: Bucher (2012)
argues that the EdgeRank algorithm, used by Facebook to determine which status updates get
prominently displayed on a users' news feed, encourages a "participatory subjectivity" in users,
who recognize that gestures of affinity (such as commenting on a friends' photo) are a key
criteria in Facebook's algorithm. Longford (2005) argues that the code of commercial platform
"habituates" us, through incessant requests and carefully designed default settings, toward giving
over more of our personal information. Mager (2012) and van Couvering (2010) both propose
that the principles of capitalism are embedded in the workings of search engines.
But we need not resort to such muscular theories of ideological domination to suggest
that algorithms designed to offer relevant knowledge also offer ways of knowing -- and that as
they become more pervasive and trusted, their logics are self-affirming. Google's search engine,
amidst its 200 signals, does presume that relevant knowledge is assured largely by public
ratification, adjusted to weigh heavily the opinions of those who are themselves publicly ratified.
This blend of the wisdom of crowds and collectively certified authorities is Google's solution to 
the longstanding tension between expertise and common sense, in the enduring problem of how
to know. It is not without precedent, and it is not a fundamentally flawed way to know, but it is a
specific one, with its own emphases and myopias. Now, their solution is operationalized into a
tool that billions of people use every day, most of whom experience it as simply working. To
some degree, Google and its algorithm help assert and normalize this knowledge logic as "right,"
as right as its results appear to be.
The Production of Calculated Publics
Ito, boyd, and others have recently introduced the term "networked publics" (boyd 2010; Ito
2008; Varnelis 2008) to highlight both the communities of users that can assemble through social
media, and the way the technologies structure how these publics can form, interact, and
sometimes fall apart. "While networked publics share much in common with other types of
publics, the ways in which technology structures them introduces distinct affordances that shape
how people engage with these environments" (boyd 2010, 39). To the extent that algorithms are
a key technological component of these mediated environments, they too help structure the
publics that can emerge using digital technology.
Some concerns have been raised about how the workings of information algorithms, and
the ways we choose to navigate them, could undermine our efforts to be involved citizens. The
ability to personalize search results and online news was the first and perhaps best articulated of
these concerns. With contemporary search engines, the results two users get to the same query
can be quite different; in a news service or social network, the information offerings can be
precisely tailored to the user's preferences (by the user, or the provider) such that, in practice, the
stories presented as most newsworthy may be so dissimilar from user to user that no common
object of public dialogue is even available. Sunstein (2001) and, more recently, Pariser (2011)
have argued that, when algorithmic information services can be personalized to this degree, the
diversity of public knowledge and political dialogue may be undermined. We are led, by
algorithms and our own preference for the like-minded, into "filter bubbles" (Pariser 2011),
where we find only the news we expect and the political perspectives we already hold dear.
But algorithms not only structure our interactions with others as members of networked
publics; algorithms also traffic in calculated publics that they themselves produce. When
Amazon recommends a book that "customers like you" bought, it is invoking and claiming to 
know a public with which we are invited to feel an affinity -- though the population on which it
bases these recommendations is not transparent, and is certainly not coterminous with its entire
customer base. When Facebook offers as a privacy setting that information be seen by "friends,
and friends of friends," it transforms a discrete set of users into an audience -- it is a group that
did not exist until that moment, and only Facebook knows its precise membership. These
algorithmically generated groups may overlap with, be an inexact approximation of, or have
nothing whatsoever to do with the publics that the user sought out.
Some algorithms go further, making claims about the public they purport to know, and
the users' place amidst them. I have argued elsewhere that Twitter's Trends algorithm promises
users a glimpse of what a particular public (national or regional) is talking about at that moment,
but that it is a constructed public, shaped by Twitter's specific, and largely unspecified criteria
(Gillespie 2012). Klout promises to measure users' influence across the various social media
platforms. Their measures are intuitive in their definition, but completely opaque in their
mechanisms. The friction between the "networked publics" forged by users and the "calculated
publics" offered by algorithms further complicates the dynamics of networked sociality.
With other measures of public opinion, such as polling or surveys, the central problem is
extrapolation, where a subset is presumed to stand for the entire population. With algorithms, the
population can be the entire user base, sometimes hundreds of millions of people (but only that
user base the algorithm provider has access to). Instead, the central problem here is that the
intention behind these calculated representations of the public is by no means actuarial.
Algorithms that purport to identify what is "hot" engage in a calculated approximation of a
public through their traceable activity, then report back to them what they have talked about
most. But behind this, we can ask, What is the gain for providers in making such
characterizations, and how does that shape what they're looking for? Who is being chosen to be
measured in order to produce this representation, and who is left out of the calculation? And
perhaps most importantly, how do these technologies, now not just technologies of evaluation
but of representation, help to constitute and codify the publics they claim to measure, publics that
would not otherwise exist except that the algorithm called them into existence?
These questions matter a great deal, and will matter more, to the extent that the
representations of the public produced by information algorithms get taken up, by users or by
authorities, as legitimate, and incorporated into the broader modernist project of reflexivity
(Giddens 1991). "Society is engaged in monitoring itself, scrutinizing itself, portraying itself in a 
variety of ways, and feeding the resulting understandings back into organizing its activities"
(Boyer and Hannerz 2006, 9). What Twitter claims matters to "Americans" or what Amazon says
teens read are forms of authoritative knowledge that can and will be invoked by institutions
whose aim is to regulate such populations.
The belief that such algorithms, combined with massive user data, are better at telling us
things about the nature of the public or the constitution of society, has proven alluring for
scholars as well. Social science has turned eagerly toward computational techniques, or the study
of human sociality through "big data," (Lazer et. al. 2009; for a critique, see boyd and Crawford
2012) in the hopes of enjoying the kind of insights that the biological sciences have achieved, by
algorithmically looking for needles in the digital haystacks of all this data. The approach is
seductive: having millions of data points lends a great deal of legitimacy, and the way algorithms
seem to spot patterns that researchers couldn't see otherwise is exciting. "For a certain sort of
social scientist, the traffic patterns of millions of e-mails look like manna from heaven" (Nature
2007). But this methodological approach should heed the complexities described so far,
particularly when their data is generated by commercial algorithms themselves. Computational
research techniques are not barometers of the social. They produce hieroglyphs: shaped by the
tool by which they are carved, requiring of priestly interpretation, they tell powerful but often
mythological stories -- usually in the service of the gods.
Conclusion
Understanding algorithms and their impact on public discourse, then, requires thinking not
simply about how they work, where they are deployed, or what animates them financially. This
is not simply a call to unveil their inner workings and spotlight their implicit criteria. It is a
sociological inquiry that does not interest the providers of these algorithms, who are not always
in the best position to even ask. It requires examining why algorithms are being looked to as a
credible knowledge logic, how they fall apart and are repaired when they come in contact with
the ebb and flow of public discourse, and where political assumptions might be not only etched
into their design, but constitutive of their widespread use and legitimacy.
I see the emergence of the algorithm as a trusted information tool as the latest response to
a fundamental tension of public discourse. The means by which we produce, circulate, and
consume information in a complex society must necessarily be handled through the division of
labor: some produce and select information, and the rest of us, at least in that moment, can only
take it for what it's worth. Every public medium previous to this has faced this challenge, from
town criers to newspapers to broadcasting. In each, when we turn over the provision of
knowledge to others, we are left vulnerable to their choices, methods, and subjectivities.
Sometimes this is a positive, providing expertise, editorial acumen, refined taste. But we are also
wary of the intervention, of human failings and vested interests, and find ourselves with only
secondary mechanisms of social trust by which to vouch for what is true and relevant (Shapin
1995). Their procedures are largely unavailable to us. Their procedures are unavoidably
selective, emphasizing some information and discarding others, and the choices may be
consequential. There is the distinct possibility of error, bias, manipulation, laziness, commercial
or political influence, or systemic failures. The selection process can always be an opportunity to
curate for reasons other than relevance: for propriety, for commercial or institutional self-
interest, or for political gain. Together this represents a fundamental vulnerability, one that we
can never fully resolve; we can merely build assurances as best we can.
From this perspective, we might see algorithms not just as codes with consequences, but
as the latest, socially constructed and institutionally managed mechanism for assuring public
acumen: a new knowledge logic. We might consider the algorithmic as posed against, and
perhaps supplanting, the editorial as a competing logic. The editorial logic depends on the
subjective choices of experts, themselves made and authorized through institutional processes of
training and certification, or validated by the public through the mechanisms of the market. The
algorithmic logic, by contrast, depends on the proceduralized choices of a machine, designed by
human operators to automate some proxy of human judgment or unearth patterns across
collected social traces. Both struggle with, and claim to resolve, the fundamental problem of
human knowledge: how to identify relevant information crucial to the public, through
unavoidably human means, in such a way as to be free from human error, bias, or manipulation.
Both the algorithmic and editorial approaches to knowledge are deeply important and deeply
problematic; much of the scholarship on communication, media, technology, and publics
grapples with one or both techniques and their pitfalls.
A sociological inquiry into algorithms should aspire to reveal the complex workings of
this knowledge machine, both the process by which it chooses information for users and the
social process by which it is made into a legitimate system. But there may be something, in the
end, impenetrable about algorithms. They are designed to work without human intervention, they
are deliberately obfuscated, and they work with information on a scale that is hard to
comprehend (at least without other algorithmic tools). And perhaps more than that, we want
relief from the duty of being skeptical about information we cannot ever assure for certain. These
mechanisms by which we settle (if not resolve) this problem, then, are solutions we cannot
merely rely on, but must believe in. But this kind of faith (Vaidhyanathan 2011) renders it
difficult to soberly recognize their flaws and fragilities.
So in many ways, algorithms remain outside our grasp, and they are designed to be. This
is not to say that we should not aspire to illuminate their workings and impact. We should. But
we may also need to prepare ourselves for more and more encounters with the unexpected and
ineffable associations they will sometimes draw for us, the fundamental uncertainty about who
we are speaking to or hearing, and the palpable but opaque undercurrents that move quietly
beneath knowledge when it is managed by algorithms.
